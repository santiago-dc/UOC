---
title: 'Minería de datos: PEC3 - Clasificación con árboles de decisión'
author: "Autor: Nombre estudiante"
date: "Mayo 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)
```


******
# Introducción
******
## Presentación
Esta prueba de evaluación continua cubre los Módulos 3 (Clasificación:
árboles de decisión) y el Módulo 8 (Evaluación de modelos) del programa de la asignatura.

## Competencias
Las competencias que se trabajan en esta prueba son:  

* Uso y aplicación de las TIC en el ámbito académico y profesional.
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.

## Objetivos
La correcta asimilación del Módulo 3. En esta PEC trabajaremos la generación e interpretación de un árbol de decisión con el software de prácticas. Seguiremos también con la preparación de los datos y la extracción inicial de conocimiento.

## Descripción de la PEC a realizar
La prueba está estructurada en un total de un único ejercicio práctico.

## Recursos Básicos
**Material docente proporcionado por la UOC.** 

Módulo 3 y 8 del material didáctico.

**Complementarios** 

* Los descritos para la anterior PEC.
* Fichero titanic.csv
* R package C5.0 (Decision Trees and Rule-Based Models): https://cran.r-project.org/web/packages/C50/index.html


## Criterios de valoración

Todos los ejercicios deben ser presentados de forma razonada y clara, especificando todos y cada uno de los pasos que se hayan llevado a cabo para su resolución. No se aceptará ninguna respuesta que no esté claramente justificada.

## Formato y fecha de entega
El formato de entrega es: usernameestudiant-PECn.html/doc/docx/odt/pdf.
Se recomienda la entrega en formato html y también el Rmd que genera el html entregado.
Fecha de Entrega: 20/05/2020.
Se debe entregar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en qué se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia  no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Enunciado  
******

En este ejercicio vamos a seguir los pasos del ciclo de vida de un proyecto de minería de datos, para el caso de un algoritmo de clasificación y más concretamente un árbol de decisión. Lo haremos con el archivo titanic.csv, que se encuentra adjunto en el aula. Este archivo contiene un registro por cada pasajero que viajaba en el Titanic. En las variables se caracteriza si era hombre o mujer, adulto o menor (niño), en qué categoría viajaba o si era miembro de la tripulación.

Objetivos:

*	Estudiar los datos, por ejemplo: ¿Número de registros del fichero? ¿Distribuciones de valores por variables? ¿Hay campos mal informados o vacíos?
*	Preparar los datos. En este caso ya están en el formato correcto y no es necesario discretizar ni generar atributos nuevos. Hay que elegir cuáles son las variables que se utilizarán para construir el modelo y cuál es la variable que clasifica. En este caso la variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no.
*	Instalar, si es necesario, el paquete C5.0  Se trata de una implementación más moderna del algoritmo ID3 de Quinlan. Tiene los principios teóricos del ID3 más la poda automática. Con este paquete generar un modelo de minería.
*	¿Cuál es la calidad del modelo?
*	Generar el árbol gráfico.
* Generar y extraer las reglas del modelo.
*	En función del modelo, el árbol y las reglas: ¿Cuál es el conocimiento que obtenemos?
*	Probar el modelo generado presentándole nuevos registros. ¿Clasifica suficientemente bien?
  
##  Revisión de los datos, extracción visual de información y preparación de los datos

Carga de los datos:

```{r message= FALSE, warning=FALSE}
data<-read.csv("./titanic.csv",header=T,sep=",")
attach(data)
```


Empezaremos haciendo un breve análisis de los datos ya que nos interesa tener una idea general de los datos que disponemos. Por ello, primero calcularemos las dimensiones de nuestra base de datos y analizaremos qué tipos de atributos tenemos.

Para empezar, calculamos las dimensiones de la base de datos mediante la función dim(). Obtenemos que disponemos de 2201 registros o pasajeros (filas) y 4 variables (columnas). 

```{r}
dim(data)
```

¿Cuáles son esas variables? Gracias a la función str() sabemos que las cuatro variables son categóricas o discretas, es decir, toman valores en un conjunto finito. La variable CLASS hace referencia a la clase en la que viajaban los pasajeros (1ª, 2ª, 3ª o crew), AGE determina si era adulto o niño (Adulto o Menor), la variable SEX si era hombre o mujer (Hombre o Mujer) y la última variable (SURVIVED) informa si el pasajero murió o sobrevivió en el accidente (Muere o Sobrevive).

```{r}
str(data)
```

Es de gran interés saber si tenemos muchos valores nulos (campos vacíos) y la distribución de valores por variables. Es por ello recomendable empezar el análisis con una visión general de las variables. Mostraremos para cada atributo la cantidad de valores perdidos mediante la función summary.  

```{r}
summary(data)
```

Disponemos por tanto de un data frame formado por cuatro variables categóricas sin valores nulos. Para un conocimiento mayor sobre los datos, tenemos a nuestro alcance unas herramientas muy valiosas: las herramientas de visualización. Para dichas visualizaciones, haremos uso de los paquetes ggplot2, gridExtra y grid de R. 

```{r}
if(!require(ggplot2)){
    install.packages('ggplot2', repos='http://cran.us.r-project.org')
    library(ggplot2)
}
if(!require(grid)){
    install.packages('grid', repos='http://cran.us.r-project.org')
    library(grid)
}
if(!require(gridExtra)){
    install.packages('gridExtra', repos='http://cran.us.r-project.org')
    library(gridExtra)
}

```


Nos interesa describir la relación entre la supervivencia y cada uno de las variables mencionadas anteriormente. Para ello, por un lado graficaremos mediante diagramas de barras la cantidad de muertos y supervivientes según la clase en la que viajaban, la edad o el sexo. Por otro lado, para obtener los datos que estamos graficando utilizaremos el comando table para dos variables que nos proporciona una tabla de contingencia.

```{r}
grid.newpage()
plotbyClass<-ggplot(data,aes(CLASS,fill=SURVIVED))+geom_bar() +labs(x="Class", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Class")
plotbyAge<-ggplot(data,aes(AGE,fill=SURVIVED))+geom_bar() +labs(x="Age", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Age")
plotbySex<-ggplot(data,aes(SEX,fill=SURVIVED))+geom_bar() +labs(x="Sex", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Sex")
grid.arrange(plotbyClass,plotbyAge,plotbySex,ncol=2)

```

De estos gráficos obtenemos información muy valiosa que complementamos con las tablas de contingencia (listadas abajo). Por un lado, la cantidad de pasajeros que sobrevivieron es similar en hombres y mujeres (hombres: 367 y mujeres 344). No, en cambio, si tenemos en cuenta el porcentaje respecto a su sexo. Es decir, pese a que la cantidad de mujeres y hombres que sobrevivieron es pareja, viajaban más hombres que mujeres (470 mujeres y 1731 hombres), por lo tanto, la tasa de muerte en hombres es muchísimo mayor (el 78,79% de los hombres murieron mientras que en mujeres ese porcentaje baja a 26,8%). 

En cuanto a la clase en la que viajaban, los pasajeros que viajaban en primera clase fueron los únicos que el porcentaje de supervivencia era mayor que el de mortalidad. El 62,46% de los viajeros de primera clase sobrevivió, el 41,4% de los que viajaban en segunda clase mientras que de los viajeros de tercera y de la tripulación solo sobrevivieron un 25,21% y 23,95% respectivamente. Para finalizar, destacamos que la presencia de pasajeros adultos era mucho mayor que la de los niños (2092 frente a 109) y que la tasa de supervivencia en niños fue mucho mayor (52,29% frente a 31,26%), no podemos obviar, en cambio, que los únicos niños que murieron fueron todos pasajeros de tercera clase (52 niños). 

```{r}
tabla_SST <- table(SEX, SURVIVED)
tabla_SST
prop.table(tabla_SST, margin = 1)
```

```{r}
tabla_SCT <- table(CLASS,SURVIVED)
tabla_SCT
prop.table(tabla_SCT, margin = 1)
```

```{r}
tabla_SAT <- table(AGE,SURVIVED)
tabla_SAT
prop.table(tabla_SAT, margin = 1) 
```

```{r}
tabla_SAT.byClass <- table(AGE,SURVIVED,CLASS)
tabla_SAT.byClass
```

Una alternativa interesante a las barras de diagramas, es el plot de las tablas de contingencia. Obtenemos la misma información pero para algunos receptores puede resultar más visual.  

```{r}
par(mfrow=c(2,2))
plot(tabla_SCT, col = c("black","#008000"), main = "SURVIVED vs. CLASS")
plot(tabla_SAT, col = c("black","#008000"), main = "SURVIVED vs. AGE")
plot(tabla_SST, col = c("black","#008000"), main = "SURVIVED vs. SEX")
```

Nuestro objetivo es crear un árbol de decisión que permita analizar qué tipo de pasajero del Titanic tenía probabilidades de sobrevivir o no. Por lo tanto, la variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no. De todas maneras, al imprimir las primeras (con head) y últimas 10 (con tail) filas nos damos cuenta de que los datos están ordenados.

```{r}
head(data,10)
tail(data,10)
```

Nos interesará "desordenarlos". Guardaremos los datos con el nuevo nombre como "data_random".

```{r}
set.seed(1)
data_random <- data[sample(nrow(data)),]
```

Para la futura evaluación del árbol de decisión, es necesario dividir el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba. El conjunto de entrenamiento es el subconjunto del conjunto original de datos utilizado para construir un primer modelo; y el conjunto de prueba, el subconjunto del conjunto original de datos utilizado para evaluar la calidad del modelo. 

Lo más correcto será utilizar un conjunto de datos diferente del que utilizamos para construir el árbol, es decir, un conjunto diferente del de entrenamiento. No hay ninguna proporción fijada con respecto al número relativo de componentes de cada subconjunto, pero la más utilizada acostumbra a ser 2/3 para el conjunto de entrenamiento y 1/3, para el conjunto de prueba. 

La variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no, que está en la cuarta columna.

```{r}
set.seed(666)
y <- data_random[,4] 
X <- data_random[,1:3] 
```


Podemos elegir el subconjunto de entrenamiento y de prueba de diversas maneras. La primer opción consiste en calcular a cuántas filas corresponde dos tercios de los datos (2*2201/3=1467) y dividir "manualmente" el conjunto.

```{r}
trainX <- X[1:1467,]
trainy <- y[1:1467]
testX <- X[1468:2201,]
testy <- y[1468:2201]
```

En la segunda opción podemos crear directamente un rango.

```{r}
indexes = sample(1:nrow(data), size=floor((2/3)*nrow(data)))
trainX<-X[indexes,]
trainy<-y[indexes]
testX<-X[-indexes,]
testy<-y[-indexes]
```

Después de una extracción aleatoria de casos es altamente recomendable efectuar un análisis de datos mínimo para asegurarnos de no obtener clasificadores sesgados por los valores que contiene cada muestra. 

## Creación del modelo, calidad del modelo y extracción de reglas

Se crea el árbol de decisión usando los datos de entrenamiento:

```{r}
model <- C50::C5.0(trainX, trainy,rules=TRUE )
summary(model)
```

Errors muestra el número y porcentaje de casos mal clasificados en el subconjunto de entrenamiento. El árbol obtenido clasifica erróneamente 304 de los 1467 casos dados, una tasa de error del 20.7%.

A partir del árbol de decisión de dos hojas que hemos modelado, se pueden extraer las siguientes reglas de decisión (gracias a rules=TRUE podemos imprimir las reglas directamente):

SEX = "Hombre" → Muere. Validez: 80,2%

CLASS = "3a" → Muere. Validez: 75.1%

CLASS "1ª", "2ª" o "Crew" y SEX = "Mujer" → Sobrevive. Validez: 90,5%

Por tanto podemos concluir que el conocimiento extraído y cruzado con el análisis visual se resume en "las mujeres y los niños primero a excepción de que fueras de 3ª clase".

A continuación mostramos el árbol obtenido.

```{r}
model <- C50::C5.0(trainX, trainy)
plot(model)
```


## Validación del modelo con los datos reservados
Una vez tenemos el modelo, podemos comprobar su calidad prediciendo la clase para los datos de prueba que nos hemos reservado al principio. 

```{r}
predicted_model <- predict( model, testX, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))
```

Cuando hay pocas clases, la calidad de la predicción se puede analizar mediante una matriz de confusión que identifica los tipos de errores cometidos. 

```{r}
mat_conf<-table(testy,Predicted=predicted_model)
mat_conf
```

Otra manera de calcular el porcentaje de registros correctamente clasificados usando la matriz de confusión:

```{r}

porcentaje_correct<-100 * sum(diag(mat_conf)) / sum(mat_conf)
print(sprintf("El %% de registros correctamente clasificados es: %.4f %%",porcentaje_correct))

```

Además, tenemos a nuestra disposición el paquete gmodels para obtener información más completa:

```{r}
if(!require(gmodels)){
    install.packages('gmodels', repos='http://cran.us.r-project.org')
    library(gmodels)
}
```
```{r}
CrossTable(testy, predicted_model,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Reality', 'Prediction'))
```


******
# Ejercicios
******

## Ejercicio 1:  
Partiendo del ejemplo mostrado, repetid el ejercicio con otro conjunto de datos. Pueden ser datos reales de vuestro ámbito laboral o de algún repositorio de datos de Internet. Mirad por ejemplo: http://www.ics.uci.edu/~mlearn/MLSummary.html i http://www.kaggle.com.

Es muy importante seleccionar correctamente el conjunto de datos y explicar de forma correcta la base de datos y la razón de su elección.

Podéis añadir o variar los puntos si lo consideráis necesario (por ejemplo, crear el modelo con todos los datos y validación cruzada, probar el boosting o variar el prunning ...) Recordad también que el ciclo de vida de los proyectos de minería contempla retroceder para volver a generar el modelo con datos modificados o parámetros del algoritmo variados si el resultado no es lo suficientemente bueno.

### Respuesta 1:

####  Revisión de los datos, extracción visual de información y preparación de los datos

Carga de los datos:

```{r message= FALSE, warning=FALSE}
df<-read.csv("./zoo.csv",header=T,sep=",")
attach(data)
```
```{r message= FALSE, warning=FALSE}
dim(df)
```
```{r message= FALSE, warning=FALSE}
str(df)
```
```{r message= FALSE, warning=FALSE}
summary(df)
```
Removemos la primera columna, pues los nombres de los animales alteraría nuestro resultado final, sería muy bueno pero no tendría mérito alguno.

```{r message= FALSE, warning=FALSE}
df<-df[,2:18]
head(df)
```
Comprobamos valores nulos.No hay.
```{r message= FALSE, warning=FALSE}
colSums(is.na(df))
colSums(df==" ")
```
En las anteriores salidas, se puede ver que todas las varibale sparecen categóricas. Hacemos una comprobación de esto.
```{r message= FALSE, warning=FALSE}
lapply(df, table)
```
Vemos que salvo la variable "legs", todas las clases solo tienen un rango de 2. Esto nos revela que probablemente sea muy complicado la discretización. Aunque por otra parte, si alguna variable que no sea legs se puede discretizar, eso implicará que se puede eliminar, quitando dimensionalidad y complejidad a nuestro problema. 


Hacemos un análisis de todas las variables en relación con la variable "class_type" para ver si podemos obtener algo de provecho.

Utilizaremos las tablas de contingencia como herramienta para visualizar los datos, establecemos la variable "class_type" como eje x para que se vea mejor.
```{r message= FALSE, warning=FALSE}
tabla_1 <- table(df$class_type,df$hair)
tabla_2 <- table(df$class_type,df$feathers)
tabla_3 <- table(df$class_type,df$eggs)
tabla_4 <- table(df$class_type,df$milk)
tabla_5 <- table(df$class_type,df$airborne)
tabla_6 <- table(df$class_type,df$aquatic)
tabla_7 <- table(df$class_type,df$predator)
tabla_8 <- table(df$class_type,df$toothed)
tabla_9 <- table(df$class_type,df$backbone)
tabla_10 <- table(df$class_type,df$breathes)
tabla_11 <- table(df$class_type,df$venomous)
tabla_12 <- table(df$class_type,df$fins)
tabla_13 <- table(df$class_type,df$legs)
tabla_14 <- table(df$class_type,df$tail)
tabla_15 <- table(df$class_type,df$domestic)
tabla_16 <- table(df$class_type,df$catsize)

par(mfrow=c(2,2))
plot(tabla_1, col = c("black","#008000"), main = "HAIR vs. CLASS")
plot(tabla_2, col = c("black","#008000"), main = "FEATHERS vs. CLASS")
plot(tabla_3, col = c("black","#008000"), main = "EGGS vs. CLASS")
plot(tabla_4, col = c("black","#008000"), main = "MILK vs. CLASS")
plot(tabla_5, col = c("black","#008000"), main = "AIRBONE vs. CLASS")
plot(tabla_6, col = c("black","#008000"), main = "AQUATIC vs. CLASS")
plot(tabla_7, col = c("black","#008000"), main = "PREDATOR vs. CLASS")
plot(tabla_8, col = c("black","#008000"), main = "TOOTHED vs. CLASS")
plot(tabla_9, col = c("black","#008000"), main = "BLACKBONE vs. CLASS")
plot(tabla_10, col = c("black","#008000"), main = "BREATHES vs. CLASS")
plot(tabla_11, col = c("black","#008000"), main = "VENOMOUS vs. CLASS")
plot(tabla_12, col = c("black","#008000"), main = "FINS vs. CLASS")
plot(tabla_13, col = c("black","#008000"), main = "LEGS vs. CLASS")
plot(tabla_14, col = c("black","#008000"), main = "TAIL vs. CLASS")
plot(tabla_15, col = c("black","#008000"), main = "DOMESTIC vs. CLASS")
plot(tabla_16, col = c("black","#008000"), main = "CATSIZE vs. CLASS")

```

Visualizamos la tabla de legs vs class para entender mejor los datos, en el plot no se ve pien porque son 5 clases vs 7.

```{r message= FALSE, warning=FALSE}
tabla_13
```

Como era de esperar, no es posible discretizar las variables sin perder infromación relevante, ninguna variable muestra homogeneidad respecto a las clases.


Nos interesará "desordenarlos". Guardaremos los datos con el nuevo nombre como "df_random".

```{r}
set.seed(1)
df_random <- df[sample(nrow(df)),]
```
```{r}
set.seed(666)
y <- as.factor(df_random[,17]) 
X <- df_random[,1:16] 
```
```{r message= FALSE, warning=FALSE}
indexes = sample(1:nrow(df), size=floor((2/3)*nrow(df)))
x_train<-X[indexes,]
y_train<-y[indexes]
x_test<-X[-indexes,]
y_test<-y[-indexes]
length(y_train)
length(y_test)
```
Evaluaremos el árbol en 34 casos. Lo entrenaremos con 67.

#### Creación del modelo, calidad del modelo y extracción de reglas

```{r message= FALSE, warning=FALSE}
model <- C50::C5.0(x_train, y_train,rules=TRUE )
summary(model)
```
Vemos que de la muestra de entrenamiento ha sido todo un éxito, con un 0% de fallo. esto es debido a que, a diferencia de en el ejemplo del titanic, aquí hay 7 clases en lugar de dos. Al tratarse de un tema como los animales, es normal que el entrenamiento haya ido tan bien, pues pocos son los animales que no cuadran en muchas características con los de sus demás especies, como podría ser el ornitorrinco, un mamífero que pone huevos.

A partir del árbol de decisión modelado podemos extraer las siguientes 7 reglas de decisión:

* Regla 1. Si el animal bebe leche, pertenece a las clase 1 con una confianza de 0.96, probablemente la clase 1 sea la de los mamíferos.
* Regla 2. Si tiene plumas, pertenece a la segunda clase, con una confianza de 0.929.
* Regla 3. si no tiene plumas, ni bebe leche, ni aletas y tiene cola, pertenece a la clase 3 con una confianza del 0.833.
* Regla 4. si no bebe leche y tiene aletas, es de la clase 4 con una confianza de 0.923.
* Regla 5. Si tiene dientes, pero no bebe leche ni tiene cola, pertenece a la clase 5 con confianza de 0.75
* Regla 6. Si es volador y no tiene cola, entonces clase 6, con una confianza de 0.875.
* Regla 7. Si no vuela, no tiene cola ni dientes, pertenece a la clase 7. confianza igual a 0.909.

```{r message= FALSE, warning=FALSE}
model <- C50::C5.0(x_train, y_train)
plot(model)
```

#### Validación del modelo con los datos reservados

Procedemos ahora a evaluar la calidad del modelo con los datos de prueba.


```{r message= FALSE, warning=FALSE}
predicted_model <- predict( model, x_test, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == y_test) / length(predicted_model)))
```

Este incremento en el error en comparación con el entrenamiento se debe a que hay ciertos casos que no se han recogido en el entrenamiento a pesar de haber barajado los datos.

En algunos animales como el delfín o la ballena, acuáticos que no beben leche, pero son mamíferos, no es posible generar una regla para ellos sin que contradiga a otra ya existente.


```{r message= FALSE, warning=FALSE}
mat_conf<-table(y_test,Predicted=predicted_model)
mat_conf
```

Vemos que por lo general, los errores se encuentran en una o dos clases, mientras que las restantes muestran un 100% de precisión.

Probamos ahora con testeando con el 100% de los datos.
```{r message= FALSE, warning=FALSE}
y <- as.factor(df_random[,17]) 
X <- df_random[,1:16] 

model2 <- C50::C5.0(X, y,rules=TRUE)
predicted_model <- predict( model2, X, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == y) / length(predicted_model)))
```

Esto se debe, a que al haber entrenado con el dataset entero, el modelo ha podido definir reglas y matizarlas para los casos atípicos, los cuales nos daban fallo en la anterior prueba. Solo falla en un caso.

## Otro modelo

En este apartado pobaremos otro modelo de cara a la clasificación de animales. Escogemos el modelo C4.5, visto en la teoría.

```{r message= FALSE, warning=FALSE}
cols<-colnames(df)
df2<- lapply(df[cols], factor)
```

```{r message= FALSE, warning=FALSE}
library(RWeka)
library(ipred)
#Bagging CART

fit <- bagging(class_type~., data=data.frame(df2))
predictions <- predict(fit, df2[1:17])

```
```{r message= FALSE, warning=FALSE}
table(predictions, df2$class_type)
```

De nuevo la precisión es muy alta, esta vez incluso superior al modelo anterior.

Esto es debido a que ha diferencia del C50, en el CART, el procedimiento de construcción del árbol pasa por considerar en cada momento el hecho de encontrar el atributo que actúa mejor como separador, lo que nos ha beneficiado en el único caso de error que teníamos.

### Conclusiones

Los árboles de decisión son un método de clasificación muy potente que además nos proporciona reglas de clasificación. Representan una venzaja en comparación con el método generador de reglas de asociación de la anterior práctica.

Resulta destacable que a penas haya diferencias de rendimiento entre los métodos presentados en esta práctica, esto puede justificarse debido a la naturaleza de nuestro set de datos. El cual, al ser una dataframe con información de animales, ignora en gran parte el factor de aleatoriedad que se podrían presentar en otros sets como el del titanic. No obstante, este nos permite no solo comprobar el funcionamiento de ambos algortimos si no también su eficacia.


# Bibliografía

* Dataset: https://archive.ics.uci.edu/ml/datasets/Zoo
* https://stackoverflow.com/questions/6286313/remove-an-entire-column-from-a-data-frame-in-r
* https://stackoverflow.com/questions/13383840/select-multiple-columns-in-data-table-by-their-numeric-indices/13383995
* https://stackoverflow.com/questions/13312477/count-unique-categorical-values-in-r
* https://www.kaggle.com/c/can-we-predict-voting-outcomes/discussion/21518
* https://stackoverflow.com/questions/31029592/c5-0-models-require-a-factor-outcome
* https://www.statmethods.net/advgraphs/layout.html
* rulequest.com/see5-unix.html
* https://topepo.github.io/C5.0/reference/C5.0.html
* https://topepo.github.io/C5.0/reference/plot.C5.0.html
* https://stat.ethz.ch/R-manual/R-devel/library/base/html/colnames.html
* https://stackoverflow.com/questions/34865798/r-weka-j48-decision-tree-cannot-handle-numeric-class
* https://rdrr.io/cran/RKEEL/man/C45-C.html
* https://stackoverflow.com/questions/4227223/convert-a-list-to-a-data-frame


******
# Rúbrica
******
* 15% Se explica de forma clara la base de datos seleccionada y la razón de su elección.
* 10% Hay un estudio sobre los datos de los que se parte y los datos son preparados correctamente.
* 20% Se aplica un árbol de decisión de forma correcta y se obtiene una estimación del error.
* 5% Se muestra de forma gráfica el árbol obtenido.
* 10% Se explican las reglas que se obtienen.
* 10% Se usa el modelo para predecir con muestras no usadas en el entrenamiento y se obtiene una estimación del error.
* 15% Se prueba otro modelo de árbol o variantes diferentes del C50 obteniendo mejores resultados.	
* 5% Se presenta el código y es fácilmente reproducible.
* 10% Se presenta unas conclusiones donde se expone el conocimiento adquirido tras el trabajo realizado.

